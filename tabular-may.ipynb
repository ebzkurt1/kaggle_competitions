{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, QuantileTransformer\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-14T15:36:21.513757Z","iopub.execute_input":"2022-05-14T15:36:21.514044Z","iopub.status.idle":"2022-05-14T15:36:21.528170Z","shell.execute_reply.started":"2022-05-14T15:36:21.514012Z","shell.execute_reply":"2022-05-14T15:36:21.527292Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv\n/kaggle/input/tabular-playground-series-may-2022/train.csv\n/kaggle/input/tabular-playground-series-may-2022/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"sample_submission_dir = '/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv'\ntrain_data_dir = '/kaggle/input/tabular-playground-series-may-2022/train.csv'\ntest_data_dir = '/kaggle/input/tabular-playground-series-may-2022/test.csv'\n\ntrain_df = pd.read_csv(train_data_dir)\ntest_df = pd.read_csv(test_data_dir)\ncombined_df = pd.concat([train_df,test_df])","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:14:00.734830Z","iopub.execute_input":"2022-05-14T15:14:00.735564Z","iopub.status.idle":"2022-05-14T15:14:08.498550Z","shell.execute_reply.started":"2022-05-14T15:14:00.735525Z","shell.execute_reply":"2022-05-14T15:14:08.497796Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(\"Train data shape : \", train_df.shape)\nprint(\"Test data shape : \", test_df.shape)\nprint(\"Overall data shape : \",combined_df.shape)\ndisplay(train_df.head())\ndisplay(train_df.info())\ndisplay(train_df.describe().transpose())","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:14:08.500076Z","iopub.execute_input":"2022-05-14T15:14:08.500323Z","iopub.status.idle":"2022-05-14T15:14:09.671581Z","shell.execute_reply.started":"2022-05-14T15:14:08.500289Z","shell.execute_reply":"2022-05-14T15:14:09.670753Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Train data shape :  (900000, 33)\nTest data shape :  (700000, 32)\nOverall data shape :  (1600000, 33)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   id      f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n0   0 -1.373246  0.238887 -0.243376  0.567405 -0.647715  0.839326  0.113133   \n1   1  1.697021 -1.710322 -2.230332 -0.545661  1.113173 -1.552175  0.447825   \n2   2  1.681726  0.616746 -1.027689  0.810492 -0.609086  0.113965 -0.708660   \n3   3 -0.118172 -0.587835 -0.804638  2.086822  0.371005 -0.128831 -0.282575   \n4   4  1.148481 -0.176567 -0.664871 -1.101343  0.467875  0.500117  0.407515   \n\n   f_07  f_08  ...      f_22      f_23      f_24      f_25      f_26  \\\n0     1     5  ... -2.540739  0.766952 -2.730628 -0.208177  1.363402   \n1     1     3  ...  2.278315 -0.633658 -1.217077 -3.782194 -0.058316   \n2     1     0  ... -1.385775 -0.520558 -0.009121  2.788536 -3.703488   \n3     3     2  ...  0.572594 -1.653213  1.686035 -2.533098 -0.608601   \n4     3     3  ... -3.912929 -1.430366  2.127649 -3.306784  4.371371   \n\n         f_27        f_28  f_29  f_30  target  \n0  ABABDADBAB   67.609153     0     0       0  \n1  ACACCADCEB  377.096415     0     0       1  \n2  AAAEABCKAD -195.599702     0     2       1  \n3  BDBBAACBCB  210.826205     0     0       1  \n4  BDBCBBCHFE -217.211798     0     1       1  \n\n[5 rows x 33 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>f_00</th>\n      <th>f_01</th>\n      <th>f_02</th>\n      <th>f_03</th>\n      <th>f_04</th>\n      <th>f_05</th>\n      <th>f_06</th>\n      <th>f_07</th>\n      <th>f_08</th>\n      <th>...</th>\n      <th>f_22</th>\n      <th>f_23</th>\n      <th>f_24</th>\n      <th>f_25</th>\n      <th>f_26</th>\n      <th>f_27</th>\n      <th>f_28</th>\n      <th>f_29</th>\n      <th>f_30</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-1.373246</td>\n      <td>0.238887</td>\n      <td>-0.243376</td>\n      <td>0.567405</td>\n      <td>-0.647715</td>\n      <td>0.839326</td>\n      <td>0.113133</td>\n      <td>1</td>\n      <td>5</td>\n      <td>...</td>\n      <td>-2.540739</td>\n      <td>0.766952</td>\n      <td>-2.730628</td>\n      <td>-0.208177</td>\n      <td>1.363402</td>\n      <td>ABABDADBAB</td>\n      <td>67.609153</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1.697021</td>\n      <td>-1.710322</td>\n      <td>-2.230332</td>\n      <td>-0.545661</td>\n      <td>1.113173</td>\n      <td>-1.552175</td>\n      <td>0.447825</td>\n      <td>1</td>\n      <td>3</td>\n      <td>...</td>\n      <td>2.278315</td>\n      <td>-0.633658</td>\n      <td>-1.217077</td>\n      <td>-3.782194</td>\n      <td>-0.058316</td>\n      <td>ACACCADCEB</td>\n      <td>377.096415</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1.681726</td>\n      <td>0.616746</td>\n      <td>-1.027689</td>\n      <td>0.810492</td>\n      <td>-0.609086</td>\n      <td>0.113965</td>\n      <td>-0.708660</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-1.385775</td>\n      <td>-0.520558</td>\n      <td>-0.009121</td>\n      <td>2.788536</td>\n      <td>-3.703488</td>\n      <td>AAAEABCKAD</td>\n      <td>-195.599702</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>-0.118172</td>\n      <td>-0.587835</td>\n      <td>-0.804638</td>\n      <td>2.086822</td>\n      <td>0.371005</td>\n      <td>-0.128831</td>\n      <td>-0.282575</td>\n      <td>3</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.572594</td>\n      <td>-1.653213</td>\n      <td>1.686035</td>\n      <td>-2.533098</td>\n      <td>-0.608601</td>\n      <td>BDBBAACBCB</td>\n      <td>210.826205</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1.148481</td>\n      <td>-0.176567</td>\n      <td>-0.664871</td>\n      <td>-1.101343</td>\n      <td>0.467875</td>\n      <td>0.500117</td>\n      <td>0.407515</td>\n      <td>3</td>\n      <td>3</td>\n      <td>...</td>\n      <td>-3.912929</td>\n      <td>-1.430366</td>\n      <td>2.127649</td>\n      <td>-3.306784</td>\n      <td>4.371371</td>\n      <td>BDBCBBCHFE</td>\n      <td>-217.211798</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 33 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 900000 entries, 0 to 899999\nData columns (total 33 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   id      900000 non-null  int64  \n 1   f_00    900000 non-null  float64\n 2   f_01    900000 non-null  float64\n 3   f_02    900000 non-null  float64\n 4   f_03    900000 non-null  float64\n 5   f_04    900000 non-null  float64\n 6   f_05    900000 non-null  float64\n 7   f_06    900000 non-null  float64\n 8   f_07    900000 non-null  int64  \n 9   f_08    900000 non-null  int64  \n 10  f_09    900000 non-null  int64  \n 11  f_10    900000 non-null  int64  \n 12  f_11    900000 non-null  int64  \n 13  f_12    900000 non-null  int64  \n 14  f_13    900000 non-null  int64  \n 15  f_14    900000 non-null  int64  \n 16  f_15    900000 non-null  int64  \n 17  f_16    900000 non-null  int64  \n 18  f_17    900000 non-null  int64  \n 19  f_18    900000 non-null  int64  \n 20  f_19    900000 non-null  float64\n 21  f_20    900000 non-null  float64\n 22  f_21    900000 non-null  float64\n 23  f_22    900000 non-null  float64\n 24  f_23    900000 non-null  float64\n 25  f_24    900000 non-null  float64\n 26  f_25    900000 non-null  float64\n 27  f_26    900000 non-null  float64\n 28  f_27    900000 non-null  object \n 29  f_28    900000 non-null  float64\n 30  f_29    900000 non-null  int64  \n 31  f_30    900000 non-null  int64  \n 32  target  900000 non-null  int64  \ndtypes: float64(16), int64(16), object(1)\nmemory usage: 226.6+ MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"           count           mean            std          min            25%  \\\nid      900000.0  449999.500000  259807.765473     0.000000  224999.750000   \nf_00    900000.0      -0.000286       0.998888    -4.599856      -0.675490   \nf_01    900000.0       0.001165       0.999193    -4.682199      -0.675162   \nf_02    900000.0       0.001174       1.000514    -4.642676      -0.674369   \nf_03    900000.0      -0.001368       1.000175    -4.658816      -0.676114   \nf_04    900000.0      -0.000571       1.000167    -4.748501      -0.675909   \nf_05    900000.0       0.000284       0.999875    -4.750214      -0.673437   \nf_06    900000.0      -0.000709       0.999942    -4.842919      -0.674876   \nf_07    900000.0       2.031460       1.656172     0.000000       1.000000   \nf_08    900000.0       2.057998       1.590955     0.000000       1.000000   \nf_09    900000.0       2.362431       1.637706     0.000000       1.000000   \nf_10    900000.0       2.177637       1.645953     0.000000       1.000000   \nf_11    900000.0       1.803392       1.537487     0.000000       1.000000   \nf_12    900000.0       2.842373       1.762835     0.000000       2.000000   \nf_13    900000.0       2.239778       1.538426     0.000000       1.000000   \nf_14    900000.0       1.514686       1.359213     0.000000       0.000000   \nf_15    900000.0       2.101132       1.569093     0.000000       1.000000   \nf_16    900000.0       2.096713       1.560169     0.000000       1.000000   \nf_17    900000.0       1.858518       1.467675     0.000000       1.000000   \nf_18    900000.0       2.065131       1.564783     0.000000       1.000000   \nf_19    900000.0       0.308713       2.316026   -11.280941      -1.236061   \nf_20    900000.0      -0.178730       2.400494   -11.257917      -1.804612   \nf_21    900000.0      -0.156307       2.484706   -13.310146      -1.820063   \nf_22    900000.0      -0.009273       2.450797   -11.853530      -1.645585   \nf_23    900000.0      -0.369459       2.453405   -12.301097      -2.019739   \nf_24    900000.0      -0.342738       2.386941   -11.416189      -1.955956   \nf_25    900000.0       0.176549       2.416959   -11.918306      -1.440424   \nf_26    900000.0       0.357591       2.476020   -14.300577      -1.261598   \nf_28    900000.0      -0.380876     238.773054 -1229.753052    -159.427418   \nf_29    900000.0       0.345661       0.475584     0.000000       0.000000   \nf_30    900000.0       1.002654       0.818989     0.000000       0.000000   \ntarget  900000.0       0.486488       0.499818     0.000000       0.000000   \n\n                  50%            75%            max  \nid      449999.500000  674999.250000  899999.000000  \nf_00         0.001144       0.674337       4.749301  \nf_01         0.002014       0.675021       4.815699  \nf_02         0.002218       0.677505       4.961982  \nf_03        -0.002227       0.672544       4.454920  \nf_04        -0.001662       0.673789       4.948983  \nf_05        -0.000438       0.675028       4.971881  \nf_06        -0.001492       0.674749       4.822668  \nf_07         2.000000       3.000000      15.000000  \nf_08         2.000000       3.000000      16.000000  \nf_09         2.000000       3.000000      14.000000  \nf_10         2.000000       3.000000      14.000000  \nf_11         2.000000       3.000000      13.000000  \nf_12         3.000000       4.000000      16.000000  \nf_13         2.000000       3.000000      12.000000  \nf_14         1.000000       2.000000      14.000000  \nf_15         2.000000       3.000000      14.000000  \nf_16         2.000000       3.000000      15.000000  \nf_17         2.000000       3.000000      14.000000  \nf_18         2.000000       3.000000      13.000000  \nf_19         0.330249       1.880517      12.079667  \nf_20        -0.190571       1.444508      11.475325  \nf_21        -0.152668       1.507071      14.455426  \nf_22         0.030850       1.661676      11.344080  \nf_23        -0.390966       1.255408      12.247100  \nf_24        -0.340746       1.266673      12.389844  \nf_25         0.160912       1.795928      12.529179  \nf_26         0.404212       2.028219      12.913041  \nf_28        -0.519808     158.987357    1229.562577  \nf_29         0.000000       1.000000       1.000000  \nf_30         1.000000       2.000000       2.000000  \ntarget       0.000000       1.000000       1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>id</th>\n      <td>900000.0</td>\n      <td>449999.500000</td>\n      <td>259807.765473</td>\n      <td>0.000000</td>\n      <td>224999.750000</td>\n      <td>449999.500000</td>\n      <td>674999.250000</td>\n      <td>899999.000000</td>\n    </tr>\n    <tr>\n      <th>f_00</th>\n      <td>900000.0</td>\n      <td>-0.000286</td>\n      <td>0.998888</td>\n      <td>-4.599856</td>\n      <td>-0.675490</td>\n      <td>0.001144</td>\n      <td>0.674337</td>\n      <td>4.749301</td>\n    </tr>\n    <tr>\n      <th>f_01</th>\n      <td>900000.0</td>\n      <td>0.001165</td>\n      <td>0.999193</td>\n      <td>-4.682199</td>\n      <td>-0.675162</td>\n      <td>0.002014</td>\n      <td>0.675021</td>\n      <td>4.815699</td>\n    </tr>\n    <tr>\n      <th>f_02</th>\n      <td>900000.0</td>\n      <td>0.001174</td>\n      <td>1.000514</td>\n      <td>-4.642676</td>\n      <td>-0.674369</td>\n      <td>0.002218</td>\n      <td>0.677505</td>\n      <td>4.961982</td>\n    </tr>\n    <tr>\n      <th>f_03</th>\n      <td>900000.0</td>\n      <td>-0.001368</td>\n      <td>1.000175</td>\n      <td>-4.658816</td>\n      <td>-0.676114</td>\n      <td>-0.002227</td>\n      <td>0.672544</td>\n      <td>4.454920</td>\n    </tr>\n    <tr>\n      <th>f_04</th>\n      <td>900000.0</td>\n      <td>-0.000571</td>\n      <td>1.000167</td>\n      <td>-4.748501</td>\n      <td>-0.675909</td>\n      <td>-0.001662</td>\n      <td>0.673789</td>\n      <td>4.948983</td>\n    </tr>\n    <tr>\n      <th>f_05</th>\n      <td>900000.0</td>\n      <td>0.000284</td>\n      <td>0.999875</td>\n      <td>-4.750214</td>\n      <td>-0.673437</td>\n      <td>-0.000438</td>\n      <td>0.675028</td>\n      <td>4.971881</td>\n    </tr>\n    <tr>\n      <th>f_06</th>\n      <td>900000.0</td>\n      <td>-0.000709</td>\n      <td>0.999942</td>\n      <td>-4.842919</td>\n      <td>-0.674876</td>\n      <td>-0.001492</td>\n      <td>0.674749</td>\n      <td>4.822668</td>\n    </tr>\n    <tr>\n      <th>f_07</th>\n      <td>900000.0</td>\n      <td>2.031460</td>\n      <td>1.656172</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>15.000000</td>\n    </tr>\n    <tr>\n      <th>f_08</th>\n      <td>900000.0</td>\n      <td>2.057998</td>\n      <td>1.590955</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>16.000000</td>\n    </tr>\n    <tr>\n      <th>f_09</th>\n      <td>900000.0</td>\n      <td>2.362431</td>\n      <td>1.637706</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>14.000000</td>\n    </tr>\n    <tr>\n      <th>f_10</th>\n      <td>900000.0</td>\n      <td>2.177637</td>\n      <td>1.645953</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>14.000000</td>\n    </tr>\n    <tr>\n      <th>f_11</th>\n      <td>900000.0</td>\n      <td>1.803392</td>\n      <td>1.537487</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>13.000000</td>\n    </tr>\n    <tr>\n      <th>f_12</th>\n      <td>900000.0</td>\n      <td>2.842373</td>\n      <td>1.762835</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>4.000000</td>\n      <td>16.000000</td>\n    </tr>\n    <tr>\n      <th>f_13</th>\n      <td>900000.0</td>\n      <td>2.239778</td>\n      <td>1.538426</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>12.000000</td>\n    </tr>\n    <tr>\n      <th>f_14</th>\n      <td>900000.0</td>\n      <td>1.514686</td>\n      <td>1.359213</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>14.000000</td>\n    </tr>\n    <tr>\n      <th>f_15</th>\n      <td>900000.0</td>\n      <td>2.101132</td>\n      <td>1.569093</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>14.000000</td>\n    </tr>\n    <tr>\n      <th>f_16</th>\n      <td>900000.0</td>\n      <td>2.096713</td>\n      <td>1.560169</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>15.000000</td>\n    </tr>\n    <tr>\n      <th>f_17</th>\n      <td>900000.0</td>\n      <td>1.858518</td>\n      <td>1.467675</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>14.000000</td>\n    </tr>\n    <tr>\n      <th>f_18</th>\n      <td>900000.0</td>\n      <td>2.065131</td>\n      <td>1.564783</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>13.000000</td>\n    </tr>\n    <tr>\n      <th>f_19</th>\n      <td>900000.0</td>\n      <td>0.308713</td>\n      <td>2.316026</td>\n      <td>-11.280941</td>\n      <td>-1.236061</td>\n      <td>0.330249</td>\n      <td>1.880517</td>\n      <td>12.079667</td>\n    </tr>\n    <tr>\n      <th>f_20</th>\n      <td>900000.0</td>\n      <td>-0.178730</td>\n      <td>2.400494</td>\n      <td>-11.257917</td>\n      <td>-1.804612</td>\n      <td>-0.190571</td>\n      <td>1.444508</td>\n      <td>11.475325</td>\n    </tr>\n    <tr>\n      <th>f_21</th>\n      <td>900000.0</td>\n      <td>-0.156307</td>\n      <td>2.484706</td>\n      <td>-13.310146</td>\n      <td>-1.820063</td>\n      <td>-0.152668</td>\n      <td>1.507071</td>\n      <td>14.455426</td>\n    </tr>\n    <tr>\n      <th>f_22</th>\n      <td>900000.0</td>\n      <td>-0.009273</td>\n      <td>2.450797</td>\n      <td>-11.853530</td>\n      <td>-1.645585</td>\n      <td>0.030850</td>\n      <td>1.661676</td>\n      <td>11.344080</td>\n    </tr>\n    <tr>\n      <th>f_23</th>\n      <td>900000.0</td>\n      <td>-0.369459</td>\n      <td>2.453405</td>\n      <td>-12.301097</td>\n      <td>-2.019739</td>\n      <td>-0.390966</td>\n      <td>1.255408</td>\n      <td>12.247100</td>\n    </tr>\n    <tr>\n      <th>f_24</th>\n      <td>900000.0</td>\n      <td>-0.342738</td>\n      <td>2.386941</td>\n      <td>-11.416189</td>\n      <td>-1.955956</td>\n      <td>-0.340746</td>\n      <td>1.266673</td>\n      <td>12.389844</td>\n    </tr>\n    <tr>\n      <th>f_25</th>\n      <td>900000.0</td>\n      <td>0.176549</td>\n      <td>2.416959</td>\n      <td>-11.918306</td>\n      <td>-1.440424</td>\n      <td>0.160912</td>\n      <td>1.795928</td>\n      <td>12.529179</td>\n    </tr>\n    <tr>\n      <th>f_26</th>\n      <td>900000.0</td>\n      <td>0.357591</td>\n      <td>2.476020</td>\n      <td>-14.300577</td>\n      <td>-1.261598</td>\n      <td>0.404212</td>\n      <td>2.028219</td>\n      <td>12.913041</td>\n    </tr>\n    <tr>\n      <th>f_28</th>\n      <td>900000.0</td>\n      <td>-0.380876</td>\n      <td>238.773054</td>\n      <td>-1229.753052</td>\n      <td>-159.427418</td>\n      <td>-0.519808</td>\n      <td>158.987357</td>\n      <td>1229.562577</td>\n    </tr>\n    <tr>\n      <th>f_29</th>\n      <td>900000.0</td>\n      <td>0.345661</td>\n      <td>0.475584</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>f_30</th>\n      <td>900000.0</td>\n      <td>1.002654</td>\n      <td>0.818989</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>target</th>\n      <td>900000.0</td>\n      <td>0.486488</td>\n      <td>0.499818</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import seaborn as sns\nsns.set_theme(style=\"whitegrid\")\nax = sns.boxplot(y=\"f_01\", data=train_df)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:14:09.673087Z","iopub.execute_input":"2022-05-14T15:14:09.673646Z","iopub.status.idle":"2022-05-14T15:14:09.864993Z","shell.execute_reply.started":"2022-05-14T15:14:09.673601Z","shell.execute_reply":"2022-05-14T15:14:09.864058Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"train_df['f_27']","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:14:09.867393Z","iopub.execute_input":"2022-05-14T15:14:09.868158Z","iopub.status.idle":"2022-05-14T15:14:09.877578Z","shell.execute_reply.started":"2022-05-14T15:14:09.868093Z","shell.execute_reply":"2022-05-14T15:14:09.876794Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"0         ABABDADBAB\n1         ACACCADCEB\n2         AAAEABCKAD\n3         BDBBAACBCB\n4         BDBCBBCHFE\n             ...    \n899995    BABBCBBBED\n899996    BBBGBBDQBE\n899997    AEBEDBBHBA\n899998    ADBAAADDAE\n899999    BCAACADSCE\nName: f_27, Length: 900000, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"for column in combined_df.columns:\n    print(f\"Number of unique elements in column {column} : \", combined_df[column].nunique())","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:14:09.879059Z","iopub.execute_input":"2022-05-14T15:14:09.879732Z","iopub.status.idle":"2022-05-14T15:14:11.945233Z","shell.execute_reply.started":"2022-05-14T15:14:09.879679Z","shell.execute_reply":"2022-05-14T15:14:11.944464Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Number of unique elements in column id :  1600000\nNumber of unique elements in column f_00 :  1600000\nNumber of unique elements in column f_01 :  1600000\nNumber of unique elements in column f_02 :  1600000\nNumber of unique elements in column f_03 :  1600000\nNumber of unique elements in column f_04 :  1600000\nNumber of unique elements in column f_05 :  1600000\nNumber of unique elements in column f_06 :  1600000\nNumber of unique elements in column f_07 :  17\nNumber of unique elements in column f_08 :  16\nNumber of unique elements in column f_09 :  16\nNumber of unique elements in column f_10 :  16\nNumber of unique elements in column f_11 :  15\nNumber of unique elements in column f_12 :  17\nNumber of unique elements in column f_13 :  14\nNumber of unique elements in column f_14 :  14\nNumber of unique elements in column f_15 :  15\nNumber of unique elements in column f_16 :  16\nNumber of unique elements in column f_17 :  15\nNumber of unique elements in column f_18 :  14\nNumber of unique elements in column f_19 :  1600000\nNumber of unique elements in column f_20 :  1600000\nNumber of unique elements in column f_21 :  1600000\nNumber of unique elements in column f_22 :  1600000\nNumber of unique elements in column f_23 :  1600000\nNumber of unique elements in column f_24 :  1600000\nNumber of unique elements in column f_25 :  1600000\nNumber of unique elements in column f_26 :  1600000\nNumber of unique elements in column f_27 :  1181880\nNumber of unique elements in column f_28 :  1600000\nNumber of unique elements in column f_29 :  2\nNumber of unique elements in column f_30 :  3\nNumber of unique elements in column target :  2\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_df['f_27'].str.split('',expand=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:14:11.946581Z","iopub.execute_input":"2022-05-14T15:14:11.946999Z","iopub.status.idle":"2022-05-14T15:14:19.468585Z","shell.execute_reply.started":"2022-05-14T15:14:11.946960Z","shell.execute_reply":"2022-05-14T15:14:19.467875Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"       0  1  2  3  4  5  6  7  8  9  10 11\n0          A  B  A  B  D  A  D  B  A  B   \n1          A  C  A  C  C  A  D  C  E  B   \n2          A  A  A  E  A  B  C  K  A  D   \n3          B  D  B  B  A  A  C  B  C  B   \n4          B  D  B  C  B  B  C  H  F  E   \n...    .. .. .. .. .. .. .. .. .. .. .. ..\n699995     B  C  B  C  E  B  H  M  C  D   \n699996     B  A  A  B  C  A  D  Q  F  C   \n699997     A  A  A  J  C  B  G  Q  B  A   \n699998     B  C  B  B  C  A  B  N  D  E   \n699999     A  F  B  E  B  A  C  H  F  F   \n\n[1600000 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td></td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>B</td>\n      <td>D</td>\n      <td>A</td>\n      <td>D</td>\n      <td>B</td>\n      <td>A</td>\n      <td>B</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td></td>\n      <td>A</td>\n      <td>C</td>\n      <td>A</td>\n      <td>C</td>\n      <td>C</td>\n      <td>A</td>\n      <td>D</td>\n      <td>C</td>\n      <td>E</td>\n      <td>B</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td></td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>E</td>\n      <td>A</td>\n      <td>B</td>\n      <td>C</td>\n      <td>K</td>\n      <td>A</td>\n      <td>D</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td></td>\n      <td>B</td>\n      <td>D</td>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>C</td>\n      <td>B</td>\n      <td>C</td>\n      <td>B</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td></td>\n      <td>B</td>\n      <td>D</td>\n      <td>B</td>\n      <td>C</td>\n      <td>B</td>\n      <td>B</td>\n      <td>C</td>\n      <td>H</td>\n      <td>F</td>\n      <td>E</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>699995</th>\n      <td></td>\n      <td>B</td>\n      <td>C</td>\n      <td>B</td>\n      <td>C</td>\n      <td>E</td>\n      <td>B</td>\n      <td>H</td>\n      <td>M</td>\n      <td>C</td>\n      <td>D</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>699996</th>\n      <td></td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>C</td>\n      <td>A</td>\n      <td>D</td>\n      <td>Q</td>\n      <td>F</td>\n      <td>C</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>699997</th>\n      <td></td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>J</td>\n      <td>C</td>\n      <td>B</td>\n      <td>G</td>\n      <td>Q</td>\n      <td>B</td>\n      <td>A</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>699998</th>\n      <td></td>\n      <td>B</td>\n      <td>C</td>\n      <td>B</td>\n      <td>B</td>\n      <td>C</td>\n      <td>A</td>\n      <td>B</td>\n      <td>N</td>\n      <td>D</td>\n      <td>E</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>699999</th>\n      <td></td>\n      <td>A</td>\n      <td>F</td>\n      <td>B</td>\n      <td>E</td>\n      <td>B</td>\n      <td>A</td>\n      <td>C</td>\n      <td>H</td>\n      <td>F</td>\n      <td>F</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n<p>1600000 rows Ã— 12 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# numerical_column_names = [c for c in combined_df.columns if combined_df[c].nunique()>20 if c not in(['id','target', 'f_27'])]\n\nf_27_label_encoder = LabelEncoder()       \nf_27_expanded = combined_df['f_27'].str.split('',expand=True)\nf_27_label_encoder.fit(np.unique(f_27_expanded.values.flatten()))\n# f_27_label_encoder.fit()\n\n# for column in f_27_expanded.columns:\n#     if column != 0 and column != 11:\n#         temp_encoder = LabelEncoder() # Initialize temporary encoders\n#         temp_encoder.fit(f_27_expanded[column])\n#         temp_name = f'f_27_{column}'\n#         f_27_label_encoders[temp_name] = temp_encoder\n\ntrain_df['f_27_set_len'] = train_df['f_27'].apply(lambda x: len(set(x)))\n        \n# numerical_columns = combined_df[numerical_column_names]\n# scaler = StandardScaler()\n# scaler.fit(numerical_columns)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:14:19.470026Z","iopub.execute_input":"2022-05-14T15:14:19.470288Z","iopub.status.idle":"2022-05-14T15:14:54.022167Z","shell.execute_reply.started":"2022-05-14T15:14:19.470253Z","shell.execute_reply":"2022-05-14T15:14:54.021370Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def memory_reduce(df):\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != 'object' and col_type != \"datetime64[ns]\":\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:14:54.026437Z","iopub.execute_input":"2022-05-14T15:14:54.028450Z","iopub.status.idle":"2022-05-14T15:14:54.043901Z","shell.execute_reply.started":"2022-05-14T15:14:54.028407Z","shell.execute_reply":"2022-05-14T15:14:54.043264Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def data_process(\n    data,\n#     numerical_column_names=numerical_column_names,\n    f_27_encoder=None,\n#     scaler=None,\n    val_split=True,\n    val_split_ratio=0.2,\n    encode=True,\n    standardize=True,\n    shrink=True,\n    drop_encoded=True,\n):\n    print(data.columns)\n    if encode:\n        expanded_f_27 = data['f_27'].str.split('',expand=True)\n        for column in expanded_f_27.columns:\n            if column != 0 and column != 11:\n                data[f'f_27_{column}'] = f_27_encoder.transform(expanded_f_27[column])\n        if drop_encoded:\n            data = data.drop('f_27',axis=1)\n    if shrink:\n        data = memory_reduce(data)\n    \n    y = data['target']\n    X = data.drop(['id','target'],axis=1)\n    if standardize:\n        quantile_scaler = QuantileTransformer(n_quantiles=100, random_state=0)\n        X = quantile_scaler.fit_transform(X)\n    if val_split:\n        X_train, X_val, y_train, y_val = train_test_split(\n            X, y, test_size=val_split_ratio, random_state=1)\n        return X_train, X_val, y_train, y_val\n    else:\n        return X, y","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:14:54.048686Z","iopub.execute_input":"2022-05-14T15:14:54.049456Z","iopub.status.idle":"2022-05-14T15:14:54.063825Z","shell.execute_reply.started":"2022-05-14T15:14:54.049420Z","shell.execute_reply":"2022-05-14T15:14:54.063069Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = data_process(\n    data=train_df,\n    f_27_encoder=f_27_label_encoder,\n#     scaler=scaler,\n    shrink=False,\n)\n# display(X_train.head())\n# display(X_train.describe().transpose())\n# display(X_train.info())\nprint(\"Transformed data sample : \\n\")\ndisplay(X_train[:1])\nprint(\"Transformed data mean : \\n\")\ndisplay(X_train.mean(axis=1))\nprint(\"Transformed data max : \\n\")\ndisplay(X_train.max(axis=1))\nprint(\"Transformed data min : \\n\")\ndisplay(X_train.min(axis=1))","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:14:54.066207Z","iopub.execute_input":"2022-05-14T15:14:54.066551Z","iopub.status.idle":"2022-05-14T15:15:05.939739Z","shell.execute_reply.started":"2022-05-14T15:14:54.066519Z","shell.execute_reply":"2022-05-14T15:15:05.938806Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Index(['id', 'f_00', 'f_01', 'f_02', 'f_03', 'f_04', 'f_05', 'f_06', 'f_07',\n       'f_08', 'f_09', 'f_10', 'f_11', 'f_12', 'f_13', 'f_14', 'f_15', 'f_16',\n       'f_17', 'f_18', 'f_19', 'f_20', 'f_21', 'f_22', 'f_23', 'f_24', 'f_25',\n       'f_26', 'f_27', 'f_28', 'f_29', 'f_30', 'target', 'f_27_set_len'],\n      dtype='object')\nTransformed data sample : \n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"array([[0.75679516, 0.8376295 , 0.82821236, 0.53978947, 0.7811149 ,\n        0.16256436, 0.06438404, 0.55050505, 0.        , 0.68181818,\n        0.85858586, 0.99158249, 0.15151515, 0.48484848, 0.68181818,\n        0.52525253, 0.52525253, 0.        , 0.74747475, 0.08844244,\n        0.37986506, 0.60548901, 0.74944548, 0.98297569, 0.60939535,\n        0.38811127, 0.99083468, 0.67455059, 1.        , 0.        ,\n        0.49494949, 0.        , 0.2979798 , 1.        , 0.75757576,\n        0.44444444, 1.        , 0.45959596, 0.28282828, 0.3030303 ,\n        0.24242424]])"},"metadata":{}},{"name":"stdout","text":"Transformed data mean : \n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"array([0.53466051, 0.51828046, 0.51940034, ..., 0.48384658, 0.37894672,\n       0.53456901])"},"metadata":{}},{"name":"stdout","text":"Transformed data max : \n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"array([1., 1., 1., ..., 1., 1., 1.])"},"metadata":{}},{"name":"stdout","text":"Transformed data min : \n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"array([0., 0., 0., ..., 0., 0., 0.])"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.linear_model import Perceptron\n\nperceptron_model = Perceptron(\n    eta0=0.1, # Learning rate of the model\n    random_state=1\n)\nperceptron_model.fit(\n    X_train,\n    y_train\n)\nperceptron_prediction = perceptron_model.predict(X_val)\nperceptron_accuracy = (perceptron_prediction==y_val).sum()/y_val.shape[0]\nprint(\"Perceptron model accuracy is : \", perceptron_accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:15:05.941334Z","iopub.execute_input":"2022-05-14T15:15:05.941596Z","iopub.status.idle":"2022-05-14T15:15:07.468001Z","shell.execute_reply.started":"2022-05-14T15:15:05.941559Z","shell.execute_reply":"2022-05-14T15:15:07.465603Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Perceptron model accuracy is :  0.5974\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nforest_model = RandomForestClassifier(\n    n_estimators=25, # Number of trees generated in the model\n    random_state=1,\n    n_jobs=2 # Making parallel computing by using 2 cores\n)\nforest_model.fit(X_train,y_train)\nforest_prediction = forest_model.predict(X_val)\nforest_accuracy = (forest_prediction == y_val).sum()/y_val.shape[0]\nprint(\"Random Forest model accuracy : \", forest_accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:15:07.469290Z","iopub.execute_input":"2022-05-14T15:15:07.469544Z","iopub.status.idle":"2022-05-14T15:16:29.939230Z","shell.execute_reply.started":"2022-05-14T15:15:07.469507Z","shell.execute_reply":"2022-05-14T15:16:29.938451Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Random Forest model accuracy :  0.8601888888888889\n","output_type":"stream"}]},{"cell_type":"code","source":"def nnmodel(inputs):\n    \"\"\"\n    Perceptron model.\n    \"\"\"\n    layer_activation = 'swish'\n    # Adding layers together. All layers consist of simple dense layers\n    layer_x = tf.keras.layers.Dense(32,name='dense_1',activation=layer_activation)(inputs)\n    # layer_x = tf.keras.layers.BatchNormalization()(layer_x)\n#     layer_x = tf.keras.layers.Dropout(0.1)(layer_x)\n\n    layer_x = tf.keras.layers.Dense(64,name='dense_2',activation=layer_activation)(layer_x)\n    # layer_x = tf.keras.layers.BatchNormalization()(layer_x)\n#     layer_x = tf.keras.layers.Dropout(0.1)(layer_x)\n\n    layer_x = tf.keras.layers.Dense(128,name='dense_3',activation=layer_activation)(layer_x)\n    # layer_x = tf.keras.layers.BatchNormalization()(layer_x)\n#     layer_x = tf.keras.layers.Dropout(0.1)(layer_x)\n\n    layer_x = tf.keras.layers.Dense(128,name='dense_4',activation=layer_activation)(layer_x)\n    # layer_x = tf.keras.layers.BatchNormalization()(layer_x)\n#     layer_x = tf.keras.layers.Dropout(0.1)(layer_x)\n\n    layer_x = tf.keras.layers.Dense(64,name='dense_5',activation=layer_activation)(layer_x)\n    # layer_x = tf.keras.layers.BatchNormalization()(layer_x)\n#     layer_x = tf.keras.layers.Dropout(0.1)(layer_x)\n\n    layer_x = tf.keras.layers.Dense(32,name='dense_6',activation=layer_activation)(layer_x)\n    # layer_x = tf.keras.layers.BatchNormalization()(layer_x)\n#     layer_x = tf.keras.layers.Dropout(0.1)(layer_x)\n\n    layer_x = tf.keras.layers.Dense(16,name='dense_7',activation=layer_activation)(layer_x)\n    # layer_x = tf.keras.layers.BatchNormalization()(layer_x)\n#     layer_x = tf.keras.layers.Dropout(0.1)(layer_x)\n    linear_reg_layer = tf.keras.layers.Dense(1,name='output_layer',activation='linear')(layer_x)\n\n    model = tf.keras.Model(\n        inputs=inputs,\n        outputs=linear_reg_layer,\n        name='linear_reg_model'\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:32:43.142169Z","iopub.execute_input":"2022-05-14T15:32:43.142429Z","iopub.status.idle":"2022-05-14T15:32:43.153519Z","shell.execute_reply.started":"2022-05-14T15:32:43.142401Z","shell.execute_reply":"2022-05-14T15:32:43.151322Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\n# Acquiring a mirrored strategy to use multiple cores on GPU\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\nBUFFER_SIZE = len(X_train)\nBATCH_SIZE_PER_REPLICA = 2048\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\ntrain_dataset_tensor = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(GLOBAL_BATCH_SIZE)\nvalidation_dataset_tensor = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(GLOBAL_BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:36:03.934565Z","iopub.execute_input":"2022-05-14T15:36:03.934849Z","iopub.status.idle":"2022-05-14T15:36:04.473712Z","shell.execute_reply.started":"2022-05-14T15:36:03.934806Z","shell.execute_reply":"2022-05-14T15:36:04.472855Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"callback_early_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=12, \n    verbose=1\n)\n\ncallback_reduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.1,\n    min_lr=1e-4,\n    patience=20,\n    verbose=1\n)\n\ncallbacks = [\n    callback_early_stopping,\n    callback_reduce_lr\n]","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:41:23.287867Z","iopub.execute_input":"2022-05-14T15:41:23.288288Z","iopub.status.idle":"2022-05-14T15:41:23.294569Z","shell.execute_reply.started":"2022-05-14T15:41:23.288249Z","shell.execute_reply":"2022-05-14T15:41:23.293708Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"model = nnmodel(tf.keras.Input(shape=(X_train.shape[-1],)))\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])\n\nmodel.fit(\n    train_dataset_tensor,\n    epochs=200,\n    validation_data=validation_dataset_tensor,\n#     batch_size=2048\n    callbacks=callbacks\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:41:23.905500Z","iopub.execute_input":"2022-05-14T15:41:23.907522Z","iopub.status.idle":"2022-05-14T15:45:56.987453Z","shell.execute_reply.started":"2022-05-14T15:41:23.907484Z","shell.execute_reply":"2022-05-14T15:45:56.986769Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Epoch 1/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.7190 - accuracy: 0.5981 - val_loss: 0.5917 - val_accuracy: 0.6938\nEpoch 2/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.5749 - accuracy: 0.7126 - val_loss: 0.5640 - val_accuracy: 0.7283\nEpoch 3/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.6242 - accuracy: 0.6617 - val_loss: 0.5765 - val_accuracy: 0.7124\nEpoch 4/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.5488 - accuracy: 0.7378 - val_loss: 0.5375 - val_accuracy: 0.7464\nEpoch 5/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.5343 - accuracy: 0.7491 - val_loss: 0.5258 - val_accuracy: 0.7588\nEpoch 6/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.5080 - accuracy: 0.7674 - val_loss: 0.4875 - val_accuracy: 0.7753\nEpoch 7/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.6263 - accuracy: 0.6650 - val_loss: 0.5443 - val_accuracy: 0.7463\nEpoch 8/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.5103 - accuracy: 0.7618 - val_loss: 0.4926 - val_accuracy: 0.7719\nEpoch 9/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.5197 - accuracy: 0.7524 - val_loss: 0.7400 - val_accuracy: 0.3872\nEpoch 10/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.6026 - accuracy: 0.7080 - val_loss: 0.5641 - val_accuracy: 0.7461\nEpoch 11/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.5054 - accuracy: 0.7785 - val_loss: 0.4765 - val_accuracy: 0.7842\nEpoch 12/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4706 - accuracy: 0.7856 - val_loss: 0.4699 - val_accuracy: 0.7860\nEpoch 13/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4662 - accuracy: 0.7877 - val_loss: 0.4662 - val_accuracy: 0.7882\nEpoch 14/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4627 - accuracy: 0.7898 - val_loss: 0.4625 - val_accuracy: 0.7906\nEpoch 15/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4590 - accuracy: 0.7919 - val_loss: 0.4589 - val_accuracy: 0.7927\nEpoch 16/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4725 - accuracy: 0.7829 - val_loss: 0.4582 - val_accuracy: 0.7948\nEpoch 17/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4540 - accuracy: 0.7958 - val_loss: 0.4529 - val_accuracy: 0.7970\nEpoch 18/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4495 - accuracy: 0.7982 - val_loss: 0.4494 - val_accuracy: 0.7982\nEpoch 19/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4447 - accuracy: 0.8010 - val_loss: 0.4427 - val_accuracy: 0.8026\nEpoch 20/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4396 - accuracy: 0.8042 - val_loss: 0.4370 - val_accuracy: 0.8060\nEpoch 21/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4456 - accuracy: 0.8018 - val_loss: 0.4685 - val_accuracy: 0.8056\nEpoch 22/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4341 - accuracy: 0.8095 - val_loss: 0.4297 - val_accuracy: 0.8113\nEpoch 23/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4277 - accuracy: 0.8120 - val_loss: 0.4236 - val_accuracy: 0.8141\nEpoch 24/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4298 - accuracy: 0.8142 - val_loss: 0.4223 - val_accuracy: 0.8160\nEpoch 25/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4161 - accuracy: 0.8179 - val_loss: 0.4169 - val_accuracy: 0.8146\nEpoch 26/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4279 - accuracy: 0.8163 - val_loss: 0.4128 - val_accuracy: 0.8203\nEpoch 27/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4082 - accuracy: 0.8216 - val_loss: 0.4075 - val_accuracy: 0.8213\nEpoch 28/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4147 - accuracy: 0.8233 - val_loss: 0.4226 - val_accuracy: 0.8226\nEpoch 29/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4416 - accuracy: 0.8082 - val_loss: 0.4198 - val_accuracy: 0.8211\nEpoch 30/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4131 - accuracy: 0.8225 - val_loss: 0.4095 - val_accuracy: 0.8225\nEpoch 31/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4103 - accuracy: 0.8225 - val_loss: 0.4043 - val_accuracy: 0.8236\nEpoch 32/200\n352/352 [==============================] - 3s 9ms/step - loss: 0.4024 - accuracy: 0.8256 - val_loss: 0.4024 - val_accuracy: 0.8243\nEpoch 33/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.5626 - accuracy: 0.7385 - val_loss: 0.6425 - val_accuracy: 0.7318\nEpoch 34/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4569 - accuracy: 0.8059 - val_loss: 0.4151 - val_accuracy: 0.8224\nEpoch 35/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4287 - accuracy: 0.8179 - val_loss: 0.5486 - val_accuracy: 0.7725\nEpoch 36/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4428 - accuracy: 0.8172 - val_loss: 0.4122 - val_accuracy: 0.8222\nEpoch 37/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4080 - accuracy: 0.8240 - val_loss: 0.4048 - val_accuracy: 0.8245\nEpoch 38/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4161 - accuracy: 0.8223 - val_loss: 0.4033 - val_accuracy: 0.8247\nEpoch 39/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4002 - accuracy: 0.8261 - val_loss: 0.3995 - val_accuracy: 0.8257\nEpoch 40/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4336 - accuracy: 0.8098 - val_loss: 0.4760 - val_accuracy: 0.8232\nEpoch 41/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4152 - accuracy: 0.8259 - val_loss: 0.3996 - val_accuracy: 0.8261\nEpoch 42/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.3954 - accuracy: 0.8277 - val_loss: 0.3936 - val_accuracy: 0.8274\nEpoch 43/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4420 - accuracy: 0.7963 - val_loss: 0.5185 - val_accuracy: 0.7720\nEpoch 44/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4386 - accuracy: 0.8124 - val_loss: 0.4200 - val_accuracy: 0.8203\nEpoch 45/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4420 - accuracy: 0.8167 - val_loss: 0.4099 - val_accuracy: 0.8224\nEpoch 46/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4592 - accuracy: 0.8001 - val_loss: 0.4653 - val_accuracy: 0.8222\nEpoch 47/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4143 - accuracy: 0.8242 - val_loss: 0.3944 - val_accuracy: 0.8269\nEpoch 48/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4056 - accuracy: 0.8246 - val_loss: 0.4205 - val_accuracy: 0.8187\nEpoch 49/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.3904 - accuracy: 0.8294 - val_loss: 0.3854 - val_accuracy: 0.8306\nEpoch 50/200\n352/352 [==============================] - 3s 9ms/step - loss: 0.3808 - accuracy: 0.8331 - val_loss: 0.3783 - val_accuracy: 0.8341\nEpoch 51/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.3760 - accuracy: 0.8359 - val_loss: 0.3711 - val_accuracy: 0.8380\nEpoch 52/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.3685 - accuracy: 0.8393 - val_loss: 0.3650 - val_accuracy: 0.8405\nEpoch 53/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.3638 - accuracy: 0.8417 - val_loss: 0.3606 - val_accuracy: 0.8419\nEpoch 54/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.3608 - accuracy: 0.8436 - val_loss: 0.3599 - val_accuracy: 0.8433\nEpoch 55/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.3558 - accuracy: 0.8461 - val_loss: 0.3562 - val_accuracy: 0.8457\nEpoch 56/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.3510 - accuracy: 0.8481 - val_loss: 0.3495 - val_accuracy: 0.8480\nEpoch 57/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.3480 - accuracy: 0.8493 - val_loss: 0.3463 - val_accuracy: 0.8488\nEpoch 58/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.3444 - accuracy: 0.8508 - val_loss: 0.3425 - val_accuracy: 0.8509\nEpoch 59/200\n352/352 [==============================] - 3s 9ms/step - loss: 0.3921 - accuracy: 0.8263 - val_loss: 0.3664 - val_accuracy: 0.8384\nEpoch 60/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.3700 - accuracy: 0.8394 - val_loss: 0.3684 - val_accuracy: 0.8381\nEpoch 61/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.3671 - accuracy: 0.8412 - val_loss: 0.3659 - val_accuracy: 0.8401\nEpoch 62/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.6285 - accuracy: 0.6769 - val_loss: 0.5213 - val_accuracy: 0.7752\nEpoch 63/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4618 - accuracy: 0.8060 - val_loss: 0.4327 - val_accuracy: 0.8173\nEpoch 64/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.4114 - accuracy: 0.8227 - val_loss: 0.3957 - val_accuracy: 0.8281\nEpoch 65/200\n352/352 [==============================] - 3s 7ms/step - loss: 0.3909 - accuracy: 0.8286 - val_loss: 0.4078 - val_accuracy: 0.8304\nEpoch 66/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.3802 - accuracy: 0.8336 - val_loss: 0.3715 - val_accuracy: 0.8378\nEpoch 67/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.6237 - accuracy: 0.6855 - val_loss: 0.5155 - val_accuracy: 0.7701\nEpoch 68/200\n352/352 [==============================] - 3s 9ms/step - loss: 0.4838 - accuracy: 0.7881 - val_loss: 0.4471 - val_accuracy: 0.8160\nEpoch 69/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.5006 - accuracy: 0.7854 - val_loss: 0.4241 - val_accuracy: 0.8184\nEpoch 70/200\n352/352 [==============================] - 3s 8ms/step - loss: 0.4262 - accuracy: 0.8203 - val_loss: 0.4011 - val_accuracy: 0.8258\nEpoch 00070: early stopping\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fc9b8dd12d0>"},"metadata":{}}]},{"cell_type":"code","source":"tf.__version__","metadata":{"execution":{"iopub.status.busy":"2022-05-14T15:13:41.860980Z","iopub.status.idle":"2022-05-14T15:13:41.861717Z","shell.execute_reply.started":"2022-05-14T15:13:41.861470Z","shell.execute_reply":"2022-05-14T15:13:41.861495Z"},"trusted":true},"execution_count":null,"outputs":[]}]}